Attentionæ˜¯ä¸€ç§ç”¨äºæå‡åŸºäºRNNï¼ˆLSTMæˆ–GRUï¼‰çš„Encoder + Decoderæ¨¡å‹æ•ˆæœçš„æœºåˆ¶ï¼ˆMechanismï¼‰ï¼Œä¸€èˆ¬ç§°ä¸ºAttention Mechanismã€‚Attentionç»™æ¨¡å‹èµ‹äºˆäº†åŒºåˆ†è¾¨åˆ«çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œåœ¨æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«åº”ç”¨ä¸­ï¼Œä¸ºå¥å­ä¸­çš„æ¯ä¸ªè¯èµ‹äºˆä¸åŒçš„æƒé‡ï¼Œä½¿ç¥ç»ç½‘ç»œæ¨¡å‹çš„å­¦ä¹ å˜å¾—æ›´åŠ çµæ´»ï¼ˆsoftï¼‰ï¼ŒåŒæ—¶Attentionæœ¬èº«å¯ä»¥åšä¸ºä¸€ç§å¯¹é½å…³ç³»ï¼Œè§£é‡Šç¿»è¯‘è¾“å…¥/è¾“å‡ºå¥å­ä¹‹é—´çš„å¯¹é½å…³ç³»ï¼Œè§£é‡Šæ¨¡å‹åˆ°åº•å­¦åˆ°äº†ä»€ä¹ˆçŸ¥è¯†ã€‚
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200303212440869.png)
ä¸Šå›¾æ˜¾ç¤ºäº†åœ¨å›¾åƒæ ‡æ³¨ä¸­çš„attentionå¯è§†åŒ–ã€‚

Attention Mechanismä¸äººç±»å¯¹å¤–ç•Œäº‹ç‰©çš„è§‚å¯Ÿæœºåˆ¶å¾ˆç±»ä¼¼ï¼Œå½“äººç±»è§‚å¯Ÿå¤–ç•Œäº‹ç‰©çš„æ—¶å€™ï¼Œä¸€èˆ¬ä¸ä¼šæŠŠäº‹ç‰©å½“æˆä¸€ä¸ªæ•´ä½“å»çœ‹ï¼Œå¾€å¾€å€¾å‘äºæ ¹æ®éœ€è¦é€‰æ‹©æ€§çš„å»è·å–è¢«è§‚å¯Ÿäº‹ç‰©çš„æŸäº›é‡è¦éƒ¨åˆ†ï¼Œæ¯”å¦‚æˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªäººæ—¶ï¼Œå¾€å¾€å…ˆAttentionåˆ°è¿™ä¸ªäººçš„è„¸ï¼Œç„¶åå†æŠŠä¸åŒåŒºåŸŸçš„ä¿¡æ¯ç»„åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå¯¹è¢«è§‚å¯Ÿäº‹ç‰©çš„æ•´ä½“å°è±¡ã€‚å› æ­¤ï¼ŒAttention Mechanismå¯ä»¥å¸®åŠ©æ¨¡å‹å¯¹è¾“å…¥çš„Xæ¯ä¸ªéƒ¨åˆ†èµ‹äºˆä¸åŒçš„æƒé‡ï¼ŒæŠ½å–å‡ºæ›´åŠ å…³é”®åŠé‡è¦çš„ä¿¡æ¯ï¼Œä½¿æ¨¡å‹åšå‡ºæ›´åŠ å‡†ç¡®çš„åˆ¤æ–­ï¼ŒåŒæ—¶ä¸ä¼šå¯¹æ¨¡å‹çš„è®¡ç®—å’Œå­˜å‚¨å¸¦æ¥æ›´å¤§çš„å¼€é”€ï¼Œè¿™ä¹Ÿæ˜¯Attention Mechanismåº”ç”¨å¦‚æ­¤å¹¿æ³›çš„åŸå› ã€‚

# ä¸€ã€Attention MechanismåŸç†
## 1.1 Attention Mechanismä¸»è¦éœ€è¦è§£å†³çš„é—®é¢˜
ã€ŠSequence to Sequence Learning with Neural Networksã€‹ä»‹ç»äº†ä¸€ç§åŸºäºRNNçš„Seq2Seqæ¨¡å‹ï¼ŒåŸºäºä¸€ä¸ªEncoderå’Œä¸€ä¸ªDecoderæ¥æ„å»ºåŸºäºç¥ç»ç½‘ç»œçš„End-to-Endçš„æœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œå…¶ä¸­ï¼ŒEncoderæŠŠè¾“å…¥Xç¼–ç æˆä¸€ä¸ªå›ºå®šé•¿åº¦çš„éšå‘é‡Cï¼ŒDecoderåŸºäºéšå‘é‡Cè§£ç å‡ºç›®æ ‡è¾“å‡ºYã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸ç»å…¸çš„åºåˆ—åˆ°åºåˆ—çš„æ¨¡å‹ï¼Œä½†æ˜¯å´å­˜åœ¨ä¸¤ä¸ªæ˜æ˜¾çš„é—®é¢˜ï¼š
1ã€æŠŠè¾“å…¥Xçš„æ‰€æœ‰ä¿¡æ¯æœ‰å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šé•¿åº¦çš„éšå‘é‡Cï¼Œå¿½ç•¥äº†è¾“å…¥Xçš„é•¿åº¦ï¼Œå½“è¾“å…¥å¥å­é•¿åº¦å¾ˆé•¿ï¼Œç‰¹åˆ«æ˜¯æ¯”è®­ç»ƒé›†ä¸­æœ€åˆçš„å¥å­é•¿åº¦è¿˜é•¿æ—¶ï¼Œæ¨¡å‹çš„æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚
2ã€æŠŠè¾“å…¥Xç¼–ç æˆä¸€ä¸ªå›ºå®šçš„é•¿åº¦ï¼Œå¯¹äºå¥å­ä¸­æ¯ä¸ªè¯éƒ½èµ‹äºˆç›¸åŒçš„æƒé‡ï¼Œè¿™æ ·åšæ˜¯ä¸åˆç†çš„ï¼Œæ¯”å¦‚ï¼Œåœ¨æœºå™¨ç¿»è¯‘é‡Œï¼Œå¯¹è¾“å…¥çš„æ¯ä¸ªè¯èµ‹äºˆç›¸åŒæƒé‡ï¼Œè¿™æ ·åšæ²¡æœ‰åŒºåˆ†åº¦ï¼Œå¾€å¾€ä½¿æ¨¡å‹æ€§èƒ½ä¸‹é™

åŒæ ·çš„é—®é¢˜ä¹Ÿå­˜åœ¨äºå›¾åƒè¯†åˆ«é¢†åŸŸï¼Œå·ç§¯ç¥ç»ç½‘ç»œCNNå¯¹è¾“å…¥çš„å›¾åƒæ¯ä¸ªåŒºåŸŸåšç›¸åŒçš„å¤„ç†ï¼Œè¿™æ ·åšæ²¡æœ‰åŒºåˆ†åº¦ï¼Œç‰¹åˆ«æ˜¯å½“å¤„ç†çš„å›¾åƒå°ºå¯¸éå¸¸å¤§æ—¶ï¼Œé—®é¢˜æ›´æ˜æ˜¾ã€‚å› æ­¤ï¼Œ2015å¹´ï¼ŒDzmitry Bahdanauç­‰äººåœ¨ã€ŠNeural machine translation by jointly learning to align and translateã€‹æå‡ºäº†Attention Mechanismï¼Œç”¨äºå¯¹è¾“å…¥Xçš„ä¸åŒéƒ¨åˆ†èµ‹äºˆä¸åŒçš„æƒé‡ï¼Œè¿›è€Œå®ç°è½¯åŒºåˆ†çš„ç›®çš„ã€‚

## 1.2 Attention MechanismåŸç†
2014å¹´åœ¨è®ºæ–‡ã€ŠSequence to Sequence Learning with Neural Networksã€‹ä¸­ä½¿ç”¨LSTMæ¥æ­å»ºSeq2Seqæ¨¡å‹ã€‚éšåï¼Œ2015å¹´ï¼ŒKyunghyun Choç­‰äººåœ¨è®ºæ–‡ã€ŠLearning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translationã€‹æå‡ºäº†åŸºäºGRUçš„Seq2Seqæ¨¡å‹ã€‚ä¸¤ç¯‡æ–‡ç« æ‰€æå‡ºçš„Seq2Seqæ¨¡å‹ï¼Œæƒ³è¦è§£å†³çš„ä¸»è¦é—®é¢˜æ˜¯ï¼Œå¦‚ä½•æŠŠæœºå™¨ç¿»è¯‘ä¸­ï¼Œå˜é•¿çš„è¾“å…¥Xæ˜ å°„åˆ°ä¸€ä¸ªå˜é•¿è¾“å‡ºYçš„é—®é¢˜ï¼Œä¸»è¦ç»“æ„å¦‚ä¸‹ï¼š
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200303212602165.png)

EncoderæŠŠä¸€ä¸ªå˜æˆçš„è¾“å…¥åºåˆ—x1ï¼Œx2ï¼Œx3....xtç¼–ç æˆä¸€ä¸ªå›ºå®šé•¿åº¦éšå‘é‡ï¼ˆèƒŒæ™¯å‘é‡ï¼Œæˆ–ä¸Šä¸‹æ–‡å‘é‡contextï¼‰cï¼Œcæœ‰ä¸¤ä¸ªä½œç”¨ï¼š
1ã€åšä¸ºåˆå§‹å‘é‡åˆå§‹åŒ–Decoderçš„æ¨¡å‹ï¼Œåšä¸ºdecoderæ¨¡å‹é¢„æµ‹y1çš„åˆå§‹å‘é‡ã€‚
2ã€åšä¸ºèƒŒæ™¯å‘é‡ï¼ŒæŒ‡å¯¼yåºåˆ—ä¸­æ¯ä¸€ä¸ªstepçš„yçš„äº§å‡ºã€‚Decoderä¸»è¦åŸºäºèƒŒæ™¯å‘é‡cå’Œä¸Šä¸€æ­¥çš„è¾“å‡ºyt-1è§£ç å¾—åˆ°è¯¥æ—¶åˆ»tçš„è¾“å‡ºytï¼Œç›´åˆ°ç¢°åˆ°ç»“æŸæ ‡å¿—ï¼ˆ\<EOS\>ï¼‰ä¸ºæ­¢ã€‚

åœ¨ä¸Šè¿°çš„æ¨¡å‹ä¸­ï¼ŒEncoder-Decoder æ¡†æ¶å°†è¾“å…¥Xéƒ½ç¼–ç è½¬åŒ–ä¸ºè¯­ä¹‰è¡¨ç¤º Cï¼Œè¿™å°±å¯¼è‡´ç¿»è¯‘å‡ºæ¥çš„åºåˆ—çš„æ¯ä¸€ä¸ªå­—éƒ½æ˜¯åŒæƒåœ°è€ƒè™‘äº†è¾“å…¥ä¸­çš„æ‰€æœ‰çš„è¯ã€‚ä¾‹å¦‚è¾“å…¥çš„è‹±æ–‡å¥å­æ˜¯ï¼šTom chase Jerryï¼Œç›®æ ‡çš„ç¿»è¯‘ç»“æœæ˜¯ï¼šæ±¤å§†è¿½é€æ°ç‘ã€‚åœ¨æœªè€ƒè™‘æ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹å½“ä¸­ï¼Œæ¨¡å‹è®¤ä¸ºâ€œæ±¤å§† â€è¿™ä¸ªè¯çš„ç¿»è¯‘å—åˆ° Tomï¼Œchase å’Œ Jerry è¿™ä¸‰ä¸ªè¯çš„åŒæƒé‡çš„å½±å“ã€‚ä½†æ˜¯å®é™…ä¸Šæ˜¾ç„¶ä¸åº”è¯¥æ˜¯è¿™æ ·å¤„ç†çš„ï¼Œâ€œæ±¤å§† â€è¿™ä¸ªè¯åº”è¯¥å—åˆ°è¾“å…¥çš„ Tom è¿™ä¸ªè¯çš„å½±å“æœ€å¤§ï¼Œè€Œå…¶å®ƒè¾“å…¥çš„è¯çš„å½±å“åˆ™åº”è¯¥æ˜¯éå¸¸å°çš„ã€‚æ˜¾ç„¶ï¼Œåœ¨æœªè€ƒè™‘æ³¨æ„åŠ›æœºåˆ¶çš„ Encoder-Decoder æ¨¡å‹ä¸­ï¼Œè¿™ç§ä¸åŒè¾“å…¥çš„é‡è¦ç¨‹åº¦å¹¶æ²¡æœ‰ä½“ç°å¤„ç†ï¼Œä¸€èˆ¬ç§°è¿™æ ·çš„æ¨¡å‹ä¸º åˆ†å¿ƒæ¨¡å‹ã€‚

è€Œå¸¦æœ‰ Attention æœºåˆ¶çš„ Encoder-Decoder æ¨¡å‹åˆ™æ˜¯è¦ä»åºåˆ—ä¸­å­¦ä¹ åˆ°æ¯ä¸€ä¸ªå…ƒç´ çš„é‡è¦ç¨‹åº¦ï¼Œç„¶åæŒ‰é‡è¦ç¨‹åº¦å°†å…ƒç´ åˆå¹¶ã€‚å› æ­¤ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥çœ‹ä½œæ˜¯ Encoder å’Œ Decoder ä¹‹é—´çš„æ¥å£ï¼Œå®ƒå‘ Decoder æä¾›æ¥è‡ªæ¯ä¸ª Encoder éšè—çŠ¶æ€çš„ä¿¡æ¯ã€‚é€šè¿‡è¯¥è®¾ç½®ï¼Œæ¨¡å‹èƒ½å¤Ÿé€‰æ‹©æ€§åœ°å…³æ³¨è¾“å…¥åºåˆ—çš„æœ‰ç”¨éƒ¨åˆ†ï¼Œä»è€Œå­¦ä¹ å®ƒä»¬ä¹‹é—´çš„â€œå¯¹é½â€ã€‚è¿™å°±è¡¨æ˜ï¼Œåœ¨ Encoder å°†è¾“å…¥çš„åºåˆ—å…ƒç´ è¿›è¡Œç¼–ç æ—¶ï¼Œå¾—åˆ°çš„ä¸åœ¨æ˜¯ä¸€ä¸ªå›ºå®šçš„è¯­ä¹‰ç¼–ç  C ï¼Œè€Œæ˜¯å­˜åœ¨å¤šä¸ªè¯­ä¹‰ç¼–ç ï¼Œä¸”ä¸åŒçš„è¯­ä¹‰ç¼–ç ç”±ä¸åŒçš„åºåˆ—å…ƒç´ ä»¥ä¸åŒçš„æƒé‡å‚æ•°ç»„åˆè€Œæˆã€‚ä¸€ä¸ªç®€å•åœ°ä½“ç° Attention æœºåˆ¶è¿è¡Œçš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼š
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200303212643111.png)
åœ¨ Attention æœºåˆ¶ä¸‹ï¼Œè¯­ä¹‰ç¼–ç  C å°±ä¸åœ¨æ˜¯è¾“å…¥Xåºåˆ—çš„ç›´æ¥ç¼–ç äº†ï¼Œè€Œæ˜¯å„ä¸ªå…ƒç´ æŒ‰å…¶é‡è¦ç¨‹åº¦åŠ æƒæ±‚å’Œå¾—åˆ°çš„ï¼Œå³ï¼š
$$C_i=\sum_{j=0}^{T_x}{a_{ij}f(x_j)}$$
åœ¨å…¬å¼ï¼ˆ6ï¼‰ä¸­ï¼Œå‚æ•° ğ‘– è¡¨ç¤ºæ—¶åˆ»ï¼Œ ğ‘—è¡¨ç¤ºåºåˆ—ä¸­çš„ç¬¬ ğ‘—ä¸ªå…ƒç´ ï¼Œ ğ‘‡ğ‘¥ è¡¨ç¤ºåºåˆ—çš„é•¿åº¦ï¼Œ ğ‘“(â‹…) è¡¨ç¤ºå¯¹å…ƒç´  ğ‘¥ğ‘—çš„ç¼–ç ã€‚ğ‘ğ‘–ğ‘—å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªæ¦‚ç‡ï¼Œåæ˜ äº†å…ƒç´  â„ğ‘— å¯¹ ğ¶ğ‘–çš„é‡è¦æ€§ï¼Œå¯ä»¥ä½¿ç”¨ softmax æ¥è¡¨ç¤ºï¼š
$$a_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}$$
è¿™é‡Œ ğ‘’ğ‘–ğ‘—æ­£æ˜¯åæ˜ äº†å¾…ç¼–ç çš„å…ƒç´ å’Œå…¶å®ƒå…ƒç´ ä¹‹é—´çš„åŒ¹é…åº¦ï¼Œå½“åŒ¹é…åº¦è¶Šé«˜æ—¶ï¼Œè¯´æ˜è¯¥å…ƒç´ å¯¹å…¶çš„å½±å“è¶Šå¤§ï¼Œåˆ™ ğ‘ğ‘–ğ‘—çš„å€¼ä¹Ÿå°±è¶Šå¤§ã€‚

å› æ­¤ï¼Œå¾—å‡º ğ‘ğ‘–ğ‘—çš„è¿‡ç¨‹å¦‚ä¸‹å›¾ï¼š
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200303213002855.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzNDE0Mjcx,size_16,color_FFFFFF,t_70)
å…¶ä¸­ï¼Œâ„i è¡¨ç¤º Encoder çš„è½¬æ¢å‡½æ•°ï¼Œğ¹(â„ğ‘—,ğ»ğ‘–)è¡¨ç¤ºé¢„æµ‹ä¸ç›®æ ‡çš„åŒ¹é…æ‰“åˆ†å‡½æ•°ã€‚å°†ä»¥ä¸Šè¿‡ç¨‹ä¸²è”èµ·æ¥ï¼Œåˆ™æ³¨æ„åŠ›æ¨¡å‹çš„ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200303213035764.png)

>â‘  å¯¹ RNN çš„è¾“å‡ºè®¡ç®—æ³¨æ„ç¨‹åº¦ï¼Œé€šè¿‡è®¡ç®—æœ€ç»ˆæ—¶åˆ»çš„å‘é‡ä¸ä»»æ„ i æ—¶åˆ»å‘é‡çš„æƒé‡ï¼Œé€šè¿‡ softmax è®¡ç®—å‡ºå¾—åˆ°æ³¨æ„åŠ›åå‘åˆ†æ•°ï¼Œå¦‚æœå¯¹æŸä¸€ä¸ªåºåˆ—ç‰¹åˆ«æ³¨æ„ï¼Œé‚£ä¹ˆè®¡ç®—çš„åå‘åˆ†æ•°å°†ä¼šæ¯”è¾ƒå¤§ã€‚  
>â‘¡ è®¡ç®— Encoder ä¸­æ¯ä¸ªæ—¶åˆ»çš„éšå‘é‡
>â‘¢ å°†å„ä¸ªæ—¶åˆ»å¯¹äºæœ€åè¾“å‡ºçš„æ³¨æ„åŠ›åˆ†æ•°è¿›è¡ŒåŠ æƒï¼Œè®¡ç®—å‡ºæ¯ä¸ªæ—¶åˆ» i å‘é‡åº”è¯¥èµ‹äºˆå¤šå°‘æ³¨æ„åŠ›
>â‘£ decoder æ¯ä¸ªæ—¶åˆ»éƒ½ä¼šå°† â‘¢ éƒ¨åˆ†çš„æ³¨æ„åŠ›æƒé‡è¾“å…¥åˆ° Decoder ä¸­ï¼Œæ­¤æ—¶ Decoder ä¸­çš„è¾“å…¥æœ‰ï¼šç»è¿‡æ³¨æ„åŠ›åŠ æƒçš„éšè—å±‚å‘é‡ï¼ŒEncoder çš„è¾“å‡ºå‘é‡ï¼Œä»¥åŠ Decoder ä¸Šä¸€æ—¶åˆ»çš„éšå‘é‡
>â‘¤ Decoder é€šè¿‡ä¸æ–­è¿­ä»£ï¼ŒDecoder å¯ä»¥è¾“å‡ºæœ€ç»ˆç¿»è¯‘çš„åºåˆ—ã€‚
>![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200306221151994.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzNDE0Mjcx,size_16,color_FFFFFF,t_70)

# äºŒã€ NMTé¢†åŸŸAttention
å‡ åå¹´æ¥ï¼Œç»Ÿè®¡æœºå™¨ç¿»è¯‘ä¸€ç›´æ˜¯å ä¸»å¯¼åœ°ä½çš„ç¿»è¯‘æ¨¡å‹ï¼Œç›´åˆ°ç¥ç»æœºå™¨ç¿»è¯‘ (NMT)çš„è¯ç”Ÿã€‚NMTæ˜¯ä¸€ç§æ–°å…´çš„æœºå™¨ç¿»è¯‘æ–¹æ³•ï¼Œå®ƒè¯•å›¾æ„å»ºå’Œè®­ç»ƒå•ä¸ªå¤§å‹çš„ç¥ç»ç½‘ç»œï¼Œæ¥è¯»å–è¾“å…¥æ–‡æœ¬å¹¶è¾“å‡ºå¯¹åº”çš„ç¿»è¯‘ã€‚
NMTçš„å…ˆé©±æ˜¯Kalchbrenner and Blunsom (2013)ï¼Œ Sutskever et. al (2014)å’ŒCho. et. al (2014b)ï¼Œå…¶ä¸­æ¯”è¾ƒç†Ÿæ‚‰çš„æ¡†æ¶æ˜¯æ¥è‡ªSutskever et. al.çš„åºåˆ—åˆ°åºåˆ—(seq2seq)æ¨¡å‹ã€‚
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200303213116290.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzNDE0Mjcx,size_16,color_FFFFFF,t_70)
ä¸Šè¿°seq2seqè¾“å…¥é•¿åº¦ä¸º4è¾“å‡ºé•¿åº¦ä¸º3ã€‚
seq2seqçš„é—®é¢˜æ˜¯ï¼Œè§£ç å™¨ä»ç¼–ç å™¨æ¥æ”¶åˆ°çš„å”¯ä¸€ä¿¡æ¯æ˜¯ç¼–ç å™¨çš„æœ€åéšè—çŠ¶æ€ï¼ˆå›¾ä¸­çš„çº¢è‰²å‘é‡ï¼‰è¿™æ˜¯ä¸€ä¸ªå‘é‡è¡¨ç¤ºï¼Œç±»ä¼¼äºè¾“å…¥åºåˆ—çš„æ•°å€¼æ‘˜è¦ã€‚åœ¨é•¿æ–‡æœ¬ä¸­ï¼Œæˆ‘ä»¬æœŸæœ›è§£ç å™¨åªä½¿ç”¨è¿™ä¸€ä¸ªå‘é‡è¡¨ç¤º(å¸Œæœ›å®ƒâ€œå……åˆ†æè¿°è¾“å…¥åºåˆ—â€)æ¥è¾“å‡ºç¿»è¯‘æ˜¯ä¸ç°å®çš„ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´ç¾éš¾æ€§çš„é—å¿˜ã€‚

å¦‚æœæˆ‘ä»¬åšä¸åˆ°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ä¸åº”è¯¥å¯¹è§£ç å™¨å¦‚æ­¤æ®‹å¿ã€‚é‚£ä¹ˆï¼Œå¦‚æœä¸å…‰ç»™ä¸€ä¸ªå‘é‡è¡¨ç¤ºï¼ŒåŒæ—¶è¿˜ç»™è§£ç å™¨ä¸€ä¸ªæ¥è‡ªæ¯ä¸ªç¼–ç å™¨æ—¶é—´æ­¥é•¿çš„å‘é‡è¡¨ç¤ºï¼Œè¿™æ ·å®ƒå°±å¯ä»¥åšå‡ºå…·æœ‰å……è¶³ä¿¡æ¯çš„ç¿»è¯‘äº†ï¼Œè¿™ä¸ªæƒ³æ³•æ€ä¹ˆæ ·ï¼Ÿè®©æˆ‘ä»¬è¿›å…¥æ³¨æ„åŠ›æœºåˆ¶ã€‚

æ³¨æ„åŠ›æœºåˆ¶æ˜¯ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„æ¥å£ï¼Œå®ƒå‘è§£ç å™¨æä¾›æ¥è‡ªæ¯ä¸ªç¼–ç å™¨éšè—çŠ¶æ€çš„ä¿¡æ¯ã€‚é€šè¿‡è¿™ä¸ªè®¾ç½®ï¼Œæ¨¡å‹èƒ½å¤Ÿé€‰æ‹©æ€§åœ°å…³æ³¨è¾“å…¥åºåˆ—çš„æœ‰ç”¨éƒ¨åˆ†ï¼Œä»è€Œå­¦ä¹ å®ƒä»¬ä¹‹é—´çš„â€œå¯¹é½â€ã€‚è¿™æœ‰åŠ©äºæ¨¡å‹æœ‰æ•ˆåœ°å¤„ç†é•¿è¾“å…¥è¯­å¥ã€‚
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200306215126380.gif)
æœ‰ä¸¤ç§æ³¨æ„ç±»å‹ï¼Œä½¿ç”¨æ‰€æœ‰ç¼–ç å™¨éšè—çŠ¶æ€çš„æ³¨æ„åŠ›ç±»å‹ä¹Ÿç§°ä¸ºâ€œå…¨å±€æ³¨æ„åŠ›â€ã€‚ç›¸åï¼Œâ€œå±€éƒ¨æ³¨æ„åŠ›â€åªä½¿ç”¨ç¼–ç å™¨éšè—çŠ¶æ€çš„å­é›†ã€‚ç”±äºæœ¬æ–‡çš„èŒƒå›´æ˜¯å…¨å±€attentionï¼Œå› æ­¤æœ¬æ–‡ä¸­æåˆ°çš„â€œattentionâ€å‡è¢«è§†ä¸ºâ€œå…¨å±€attentionâ€ã€‚


å¼•å…¥ Attention çš„ Encoder-Decoder æ¡†æ¶ä¸‹ï¼Œå®Œæˆæœºå™¨ç¿»è¯‘ä»»åŠ¡çš„å¤§è‡´æµç¨‹å¦‚ä¸‹ï¼š
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/2020030621560365.gif)

æ³¨æ„åŠ›é›†ä¸­åœ¨ä¸åŒçš„å•è¯ä¸Šï¼Œç»™æ¯ä¸ªå•è¯æ‰“åˆ†ã€‚ç„¶åï¼Œä½¿ç”¨softmaxä¹‹ååˆ†æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¼–ç å™¨éšè—çŠ¶æ€çš„åŠ æƒå’Œæ¥èšåˆç¼–ç å™¨éšè—çŠ¶æ€ï¼Œå¾—åˆ°ä¸Šä¸‹æ–‡å‘é‡ã€‚

# ä¸‰ã€ä¸»è¦ä»£ç å®ç°
## 3.1 Encoder

```python
class Encoder(tf.keras.Model):
    def __init__(self, hidden_size=1024, max_sequence_len=30, batch_size=batch_size, embedding_dim=256, vocab_size=5000):
        super(Encoder, self).__init__()
        self.embedding_dim = embedding_dim
        self.vocab_size = vocab_size
        self.max_sequence_len = max_sequence_len
        self.hidden_size = hidden_size
        self.batch_size = batch_size

        self.embedding_layer = Embedding(
            input_dim=self.vocab_size, output_dim=self.embedding_dim)
        self.GRU_1 = GRU(units=hidden_size, return_sequences=True)
        self.GRU_2 = GRU(units=hidden_size,
                         return_sequences=True, return_state=True)

    def initial_hidden_state(self):
        return tf.zeros(shape=(self.batch_size, self.hidden_size))

    def call(self, x, initial_state, training=False):
        x = self.embedding_layer(x)
        x = self.GRU_1(x, initial_state=initial_state)
        x, hidden_state = self.GRU_2(x)
        return x, hidden_state
```
## 3.2 Attention


```python
class Attention(tf.keras.Model):
    def __init__(self, hidden_size=256):
        super(Attention, self).__init__()
        self.fc1 = Dense(units=hidden_size)
        self.fc2 = Dense(units=hidden_size)
        self.fc3 = Dense(units=1)

    def call(self, encoder_output, hidden_state, training=False):
        '''hidden_state : h(t-1)'''
        y_hidden_state = tf.expand_dims(hidden_state, axis=1)
        y_hidden_state = self.fc1(y_hidden_state)
        y_enc_out = self.fc2(encoder_output)
        
        #get a_ij
        y = tf.keras.backend.tanh(y_enc_out + y_hidden_state)
        attention_score = self.fc3(y)
        attention_weights = tf.keras.backend.softmax(attention_score, axis=1)
        
        #get c_i
        context_vector = tf.multiply(encoder_output, attention_weights)
        context_vector = tf.reduce_sum(context_vector, axis=1)

        return context_vector, attention_weights
```

## 3.3 Decoder

```python
class Decoder(tf.keras.Model):
    def __init__(self, hidden_size=1024, max_sequence_len=30, batch_size=batch_size, embedding_dim=256, vocab_size=5000):
        super(Decoder, self).__init__()
        self.embedding_dim = embedding_dim
        self.vocab_size = vocab_size
        self.max_sequence_len = max_sequence_len
        self.hidden_size = hidden_size
        self.batch_size = batch_size
    
        self.embedding_layer = Embedding(
            input_dim=self.vocab_size, output_dim=self.embedding_dim)
        self.GRU = GRU(units=hidden_size,
                       return_sequences=True, return_state=True)
        self.attention = Attention(hidden_size=self.hidden_size)
        self.fc = Dense(units=self.vocab_size)

    def initial_hidden_state(self):
        return tf.zeros(shape=(self.batch_size, self.hidden_size))

    def call(self, x, encoder_output, hidden_state, training=False):
        x = self.embedding_layer(x)
        context_vector, attention_weights = self.attention(
            encoder_output, hidden_state, training=training)
        contect_vector = tf.expand_dims(context_vector, axis=1)
        x = tf.concat([x, contect_vector], axis=-1)
        x, curr_hidden_state = self.GRU(x)
        x = tf.reshape(x, shape=[self.batch_size, -1])
        x = self.fc(x)
        return x, curr_hidden_state, attention_weights
```
æœ€ç»ˆç»“æœï¼š
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/2020030321370532.png)
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20200303213607681.png)

**å®Œæ•´ä»£ç **ï¼š[https://github.com/LIANGQINGYUAN/NLP-Notebook](https://github.com/LIANGQINGYUAN/NLP-Notebook)
æ¬¢è¿starï½


å‚è€ƒé“¾æ¥ï¼š
æ¨¡å‹æ±‡æ€»24 - æ·±åº¦å­¦ä¹ ä¸­Attention Mechanismè¯¦ç»†ä»‹ç»ï¼šåŸç†ã€åˆ†ç±»åŠåº”ç”¨ï¼š
https://zhuanlan.zhihu.com/p/31547842
æµ…è°ˆ Attention æœºåˆ¶çš„ç†è§£ï¼šhttps://www.cnblogs.com/ydcode/p/11038064.html
Attentionå¯è§†åŒ–ï¼šhttps://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
Intuitive Understanding of Attention Mechanism in Deep Learningï¼š
https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f
kaggleç¿»è¯‘ä¾‹å­1ï¼šhttps://www.kaggle.com/nikhilxavier/english-to-hindi-machine-translation-attention
kaggleç¿»è¯‘ä¾‹å­2ï¼šhttps://www.kaggle.com/harishreddy18/english-to-french-translation
Go from the basics - Attention mechanism, transformers, BERTï¼š
https://www.kaggle.com/c/tensorflow2-question-answering/discussion/115676#711847
æœºå™¨ç¿»è¯‘è¯­æ–™åº“ï¼šhttp://www.manythings.org/anki/
 Transformer ç³»åˆ—ä¸€ï¼šhttps://zhuanlan.zhihu.com/p/109585084